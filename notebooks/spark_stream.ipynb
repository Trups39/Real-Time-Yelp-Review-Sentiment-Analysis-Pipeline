{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "from transformers import pipeline\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "checkpoint_dir = \"/kaggle/working/checkpoints/kafka_to_mongo\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "    \n",
    "config = {\n",
    "    \"kafka\": {\n",
    "    \"bootstrap.servers\":\"<YOUR_BOOTSTRAP_SERVERS>\",\n",
    "    \"security.protocol\":\"SASL_SSL\",\n",
    "    \"sasl.mechanisms\":\"PLAIN\",\n",
    "    \"sasl.username\":\"<KAFKA_API_KEY>\",\n",
    "    \"sasl.password\":\"<KAFKA_API_SECRET>\",\n",
    "    \"client.id\":\"json-serial-producer\"\n",
    "},\n",
    "    \"mongodb\": {\n",
    "        \"uri\":\"<YOUR_MONGODB_URI>\",\n",
    "        \"database\":\"<YOUR_DATABASE>\",\n",
    "        \"collection\":\"<YOUR_COLLECTION>\"\n",
    "    }\n",
    "}\n",
    "\n",
    "sentiment_pipeline = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    if text and isinstance(text, str):\n",
    "        try:\n",
    "            result = sentiment_pipeline(text)[0]\n",
    "            return result['label']\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in sentiment analysis: {e}\")\n",
    "            return \"Error\"\n",
    "    return \"Empty or Invalid\"\n",
    "\n",
    "sentiment_udf = udf(analyze_sentiment, StringType())\n",
    "\n",
    "def read_from_kafka_and_write_to_mongo(spark):\n",
    "    topic = \"raw_topic\"\n",
    "    \n",
    "    schema = StructType([\n",
    "        StructField(\"review_id\",StringType()),\n",
    "        StructField(\"user_id\",StringType()),\n",
    "        StructField(\"business_id\",StringType()),\n",
    "        StructField(\"stars\",FloatType()),\n",
    "        StructField(\"useful\",IntegerType()),\n",
    "        StructField(\"funny\",IntegerType()),\n",
    "        StructField(\"cool\",IntegerType()),\n",
    "        StructField(\"text\",StringType()),\n",
    "        StructField(\"date\",StringType())\n",
    "    ])\n",
    "    \n",
    "    stream_df = (spark.readStream\n",
    "                 .format(\"kafka\")\n",
    "                 .option(\"kafka.bootstrap.servers\",config['kafka']['bootstrap.servers'])\n",
    "                 .option(\"subscribe\",topic)\n",
    "                 .option(\"kafka.security.protocol\", config['kafka']['security.protocol'])\n",
    "                 .option(\"kafka.sasl.mechanism\",config['kafka']['sasl.mechanisms'])\n",
    "                 .option(\"kafka.sasl.jaas.config\",\n",
    "                        f'org.apache.kafka.common.security.plain.PlainLoginModule required username=\"{config[\"kafka\"][\"sasl.username\"]}\" '\n",
    "                        f'password=\"{config[\"kafka\"][\"sasl.password\"]}\";')\n",
    "                 .option(\"failOnDataLoss\",\"false\")\n",
    "                 .load()\n",
    "                )\n",
    "    parsed_df = stream_df.select(from_json(col('value').cast(\"string\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "    \n",
    "    enriched_df = parsed_df.withColumn(\"sentiment\", sentiment_udf(col('text')))\n",
    "    \n",
    "    query = (enriched_df.writeStream\n",
    "             .format(\"mongodb\")\n",
    "             .option(\"spark.mongodb.connection.uri\", config['mongodb']['uri'])\n",
    "             .option(\"spark.mongodb.database\", config['mongodb']['database'])\n",
    "             .option(\"spark.mongodb.collection\", config['mongodb']['collection'])\n",
    "             .option(\"checkpointLocation\", checkpoint_dir)\n",
    "             .outputMode(\"append\")\n",
    "             .start()\n",
    "             .awaitTermination()\n",
    "            )\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    spark = (SparkSession.builder\n",
    "          .appName(\"KafkaStreamToMongo\")\n",
    "          .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.2,org.mongodb.spark:mongo-spark-connector_2.12:10.4.0\")\n",
    "          .getOrCreate()\n",
    "          )\n",
    "    read_from_kafka_and_write_to_mongo(spark)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
